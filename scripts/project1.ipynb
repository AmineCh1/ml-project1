{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "import seaborn as sns\n",
    "import math\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # Path relative to your mahcine I guess ? \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test) # ????\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(x, y, i):\n",
    "    \"Merge data and labels\"\n",
    "    data = np.c_[y, x]\n",
    "    \n",
    "    \"Find indexes where the 23rd element of the data is 'i' (i = 0, 1, 2 and 3)\"\n",
    "    index = data[:, 23]==i\n",
    "    \n",
    "    \"Split the data according to the indexes found\"\n",
    "    data_i = data[index, :]\n",
    "    \"Unmerge the data and labels\"\n",
    "    y_i = data_i[:, 0]\n",
    "    x_i = data_i[:, 1:]\n",
    "    \n",
    "    \"-999s don't appear in 1st column following the indexes, so we don't take it in consideration in the following\"\n",
    "    \"and we get rid of the then 21st column containing the 'i's\"\n",
    "    x_test = np.delete(x_i[:, 1:], 21, axis=1)\n",
    "    \n",
    "    \"Remove columns where elements have value -999\"\n",
    "   \n",
    "    x_test = np.delete(x_test, np.where(x_test[0]==-999), axis=1)\n",
    "    \n",
    "    \"Concactenation of 1st column of the data and the rest with elements of value -999 removed\"\n",
    "    x_test = np.c_[x_i[:, 0], x_test]\n",
    "    \n",
    "    \"Replace elements of value -999 in 1st column of the data by the mean of \"\n",
    "    \"Here, we can do a weighted mean instead of simply taking the mean \"\n",
    "    for column in range(x_test[0].size):\n",
    "        indices = x_test[:,column]==-999\n",
    "        x_test[indices, column] = np.mean(x_test[~indices, column])\n",
    "\n",
    "    return index, y_i, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index0, y0, x_test0 = partition_data(tX, y, 0)\n",
    "index1, y1, x_test1 = partition_data(tX, y, 1)\n",
    "index2, y2, x_test2 = partition_data(tX, y, 2)\n",
    "index3, y3, x_test3 = partition_data(tX, y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr(x_test):\n",
    "    corr_mat = np.corrcoef(x_test,rowvar=False)\n",
    "    return np.where(np.abs(corr_mat)>=0.95, 1,0)\n",
    "\n",
    "def visualize_corr(corr):\n",
    "    fig, ax = plt.subplots(figsize=(corr.shape[0],corr.shape[1]))\n",
    "    sns.heatmap(corr, vmax=1.0, center=0, fmt='.2f',\n",
    "            square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
    "    plt.show();\n",
    "\n",
    "def standardize_data(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize the 4 batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std_test0 = standardize_data(x_test0)\n",
    "std_test1 = standardize_data(x_test1)\n",
    "std_test2 = standardize_data(x_test2)\n",
    "std_test3 = standardize_data(x_test3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to find any correlations between features of the standardized data, by taking a look at their correlation matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_mat_0 = compute_corr(std_test0[:,:-1])\n",
    "corr_mat_1 = compute_corr(std_test1) \n",
    "corr_mat_2 = compute_corr(std_test2)\n",
    "corr_mat_3 = compute_corr(std_test3)\n",
    "    \n",
    "corrs = [corr_mat_0,corr_mat_1,corr_mat_2,corr_mat_3]\n",
    "\n",
    "for elem in corrs:\n",
    "    visualize_corr(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that among the 4 batches of partitioned data, the first batch has a feature which is strictly correlated to the other ! \n",
    "Furthermore, for the second, thrid , and fourth batch, we observe that some features here and there have very high correlation coefficients, greater than 0.95 ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(y,tx,initial_w,max_iters=10000,gamma=0.05,model=least_squares_GD,lambda_=0):\n",
    "   \n",
    "    loss, w = model(y,tx,initial_w,max_iters,gamma) #Change for future algorithms \n",
    "   \n",
    "    return w\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_T(data,labels,prop,gamma):\n",
    "    train_prop = int(data.shape[0]*prop)\n",
    "    \n",
    "    train = data[:train_prop] \n",
    "    y_train = labels[:train_prop]\n",
    "    y_test = labels[train_prop:]\n",
    "    test  = data[train_prop:]\n",
    "    \n",
    "    rand_w = [np.random.uniform(-1,1) for x in range(data.shape[1])]\n",
    "    \n",
    "    w = run_model(y_train,train,rand_w,gamma = gamma)\n",
    "    return np.mean(predict_labels(w,test)==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_T(std_test0,y0,0.9,0.05)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(K,y_batch,tx_batch,max_iters,gamma,model):\n",
    "   \n",
    "    N = tx_batch.shape[0]\n",
    "    fold_size  = tx_batch.shape[0]//K #euclidean divison => cast as int\n",
    "    \n",
    "    accuracies = [] # nb of accuracies to compute = K \n",
    "    \n",
    "    ws = []\n",
    "    for i in range(0,tx_batch.shape[0]-1,fold_size):\n",
    "        \n",
    "        #Define the indexes of the training set\n",
    "        start = i\n",
    "        end = i + fold_size\n",
    "        \n",
    "        #Define training set\n",
    "        train_tx = np.vstack((tx_batch[:start],tx_batch[end:N-1]))\n",
    "        print(train_tx.shape)\n",
    "        train_y = np.array(list(y_batch[:start])+list(y_batch[end:N-1]))\n",
    "        print(train_y.shape)\n",
    "        \n",
    "        # for each run of the model start with a random w ( Is this a good idea ? )\n",
    "        rand_initial = [np.random.uniform(-10,10) for x in range(tx_batch.shape[1])]\n",
    "        w= run_model(train_y,train_tx,rand_initial,max_iters,gamma,model) #rewrite .... \n",
    "        \n",
    "        # Test w\n",
    "        ws.append(w)\n",
    "        acc = np.mean(predict_labels(w,tx_batch[start:end])==y[start:end])\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        print(acc)\n",
    "        \n",
    "    print(\"This is ws \\n\")\n",
    "    print(ws[-1])\n",
    "    return np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(11,y0,std_test0,max_iters=1000,gamma=0.0000001,model=least_squares_GD)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
